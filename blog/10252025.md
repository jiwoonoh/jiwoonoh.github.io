---
title: 'Mixture-of-Experts (MoE) for Medical Applications'
date: 2025-10-25
layout: single
permalink: /blog/moe-health
---
Throwback to a month ago, I did my first Machine Learning subgroup presentation, where I explained about the Mixture-of-Experts (MoE) models and their applications in medicine. 

### What is a MoE?
This blog post will explain MoE in context of Transformers. MoE is a machine learning architecture that combines multiple specialized "expert" models with a "gating network" to dynamically route inputs to the most relevant experts [^1]. In a transformer, MoE has two main elements: 
1. They have sparse MoE layers instead of dense feed-forward network (FFN) layers,
2. Router network determines which token is sent to which expert.

See image below to understand the difference between regular versus MoE transformers. 

<img src="MOE-1.png" width="800" height="800" />

#### MoE works as following:
1. Router network scores the input token against each expert and applies a softmax to produce a probability for every expert: \\
$$h_{soft}(x)_{i} = \text{softmax}(x^T W_i)$$
2. Top-K experts (here, k=1) are activated.
3. Expert’s FFN processes the token and produces an output
4. Router’s probability for that expert (e.g., 0.45) is used to weigh the expert’s output before passing it onward

<img src="MOE-2.png" width="400" height="400" />

These MoE Transformers are great at dynamic specialization, as each expert focuses on particular patterns or inputs. They also have improved model capacity as they are able to process high number parameters without comparable costs

Dynamic specialization: Each expert focuses on particular patterns or inputs

Improved model capacity: high number of parameters achieve high capacity without comparable costs
Sparse Top-k selection


[^1]: IBM. (n.d.). *What is a mixture of experts (MoE)?* IBM Think. Retrieved October 25, 2025, from https://www.ibm.com/think/topics/mixture-of-experts