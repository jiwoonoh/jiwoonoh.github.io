---
title: 'Mixture-of-Experts (MoE) for Medical Applications'
date: 2025-10-25
layout: single
permalink: /blog/moe-health
---

## Mixture-of-Experts (MoE) for Medical Applications

Throwback to a month ago, I did my first Machine Learning subgroup presentation, where I explained about the Mixture-of-Experts (MoE) models and their applications in medicine. 

### What is a MoE?
This blog post will explain MoE in context of Transformers. MoE is a machine learning architecture that combines multiple specialized "expert" models with a "gating network" to dynamically route inputs to the most relevant experts [^first]. In a transformer, MoE has two main elements: 
1. They have sparse MoE layers instead of dense feed-forward network (FFN) layers,
2. Router network determines which token is sent to which expert.


[^first] IBM. (n.d.). *What is a mixture of experts (MoE)?* IBM Think. Retrieved October 25, 2025, from https://www.ibm.com/think/topics/mixture-of-experts