---
title: 'Mixture-of-Experts (MoE) for Medical Applications'
date: 2025-10-25
layout: single
permalink: /blog/moe-health
---
Throwback to a month ago, I did my first Machine Learning subgroup presentation, where I explained about the Mixture-of-Experts (MoE) models and their applications in medicine. 

### What is a MoE?
This blog post will explain MoE in context of Transformers. MoE is a machine learning architecture that combines multiple specialized "expert" models with a "gating network" to dynamically route inputs to the most relevant experts [^1]. 

<div style="display: flex;">
  <div style="flex: 1; padding-right: 16px;">
    ![Regular vs MoE Transformers](MOE-1.png "Regular vs MoE Transformers")

  </div>
  <div style="flex: 1; padding-left: 16px;">
    In a transformer, MoE has two main elements: 
    1. They have sparse MoE layers instead of dense feed-forward network (FFN) layers,
    2. Router network determines which token is sent to which expert.
  </div>
</div>

The workflow of MoE is as follows:

<div style="display: flex;">
  <div style="flex: 1; padding-right: 16px;">
    ![MoE steps](MOE-2.png "MOE Steps")

  </div>
  <div style="flex: 1; padding-left: 16px;">
    1. Router network scores the input token against each expert and applies a softmax to produce a probability for every expert: \\
    \h_{soft}(\x)_{\i} = $\x^intercal$\W_\i
    2. Top-K experts (here, k=1) are activated.
    3. Expert’s FFN processes the token and produces an output
    4. Router’s probability for that expert (e.g., 0.45) is used to weigh the expert’s output before passing it onward
  </div>
</div>

[^1]: IBM. (n.d.). *What is a mixture of experts (MoE)?* IBM Think. Retrieved October 25, 2025, from https://www.ibm.com/think/topics/mixture-of-experts