---
title: 'Mixture-of-Experts (MoE) for Medical Applications'
date: 2025-10-25
layout: single
permalink: /blog/moe-health
---
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <!-- 1. Use the title from your text -->
    <title>Mixture-of-Experts (MoE) for Medical Applications</title>
    
    <!-- 2. Load MathJax to render the LaTeX equation -->
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <!-- 3. Minimal styles to make your layout work -->
    <style>
    </style>
</head>
<body>
    <h1>Mixture-of-Experts (MoE) for Medical Applications</h1>
    <p><em>Published: October 25, 2025</em></p>
    <hr>
    <p>Throwback to a month ago, I did my first Machine Learning subgroup presentation, where I explained about the Mixture-of-Experts (MoE) models and their applications in medicine.</p>
    <h3>What is a MoE?</h3>
    <p>This blog post will explain MoE in context of Transformers. MoE is a machine learning architecture that combines multiple specialized "expert" models with a "gating network" to dynamically route inputs to the most relevant experts [1].</p>
    <!-- Your first flex layout -->
    <div class="flex-container">
        <div class="flex-item">
            <!-- 
              Replaced Markdown image with HTML <img> tag.
              Using a placeholder since I can't access your local 'MOE-1.png'.
            -->
            <img src="https://placehold.co/400x300/F0F4F8/334155?text=Regular+vs+MoE" 
                 alt="Regular vs MoE Transformers" 
                 title="Regular vs MoE Transformers">
        </div>
        <div class="flex-item">
            <p>In a transformer, MoE has two main elements:</p> 
            <ol>
                <li>They have sparse MoE layers instead of dense feed-forward network (FFN) layers,</li>
                <li>Router network determines which token is sent to which expert.</li>
            </ol>
        </div>
    </div>
    <p>The workflow of MoE is as follows:</p>
    <!-- Your second flex layout -->
    <div class="flex-container">
        <div class="flex-item">
            <!-- 
              Replaced Markdown image with HTML <img> tag.
              Using a placeholder for 'MOE-2.png'.
            -->
            <img src="https://placehold.co/400x300/F0F4F8/334155?text=MoE+Steps" 
                 alt="MoE steps" 
                 title="MoE Steps">
        </div>
        <div class="flex-item">
            <ol>
                <li>
                    Router network scores the input token against each expert and applies a softmax to produce a probability for every expert:
                    <!-- 
                      Fixed the LaTeX syntax and wrapped it for MathJax.
                      I interpreted \x^intercal as x^T (x transpose), which is standard.
                    -->
                    <div class="math-equation">
                        \( h_{soft}(x)_{i} = \text{softmax}(x^T W_i) \)
                    </div>
                </li>
                <li>Top-K experts (here, k=1) are activated.</li>
                <li>Expert’s FFN processes the token and produces an output</li>
                <li>Router’s probability for that expert (e.g., 0.45) is used to weigh the expert’s output before passing it onward</li>
            </ol>
        </div>
    </div>
    <div class="footnote">
        <p>[1] IBM. (n.d.). <em>What is a mixture of experts (MoE)?</em> IBM Think. Retrieved October 25, 2025, from https://www.ibm.com/think/topics/mixture-of-experts</p>
    </div>

</body>
</html>
